"""
recipe_new í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ì„œ FAISS ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
ì£¼ì¬ë£Œë¥¼ ê°•ì¡°í•œ ì„ë² ë”© ìƒì„±
"""

import logging
import faiss
import numpy as np
from app.db import SessionLocal
from sentence_transformers import SentenceTransformer
import os
import pickle
from tqdm import tqdm
import torch
import gc
from sqlalchemy import text

# ì„¤ì •
CHUNK_SIZE = 1000
# ê²½ë¡œ ì„¤ì •: ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ faiss_store í´ë” ì‚¬ìš©
FAISS_STORE_DIR = "faiss_store"
if not os.path.exists(FAISS_STORE_DIR):
    # app í´ë”ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²½ìš°
    FAISS_STORE_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "faiss_store") if "__file__" in globals() else "faiss_store"

INDEX_SAVE_PATH = os.path.join(FAISS_STORE_DIR, "index_new.faiss")
META_SAVE_PATH = os.path.join(FAISS_STORE_DIR, "metadata_new.pkl")
LAST_PROCESSED_PATH = os.path.join(FAISS_STORE_DIR, "last_processed_new.txt")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ëª¨ë¸ ë¡œë“œ
logger.info("ğŸ“¦ SentenceTransformer ëª¨ë¸ ë¡œë”© ì¤‘...")
try:
    # M1 Mac ì§€ì› (CPU/MPS)
    device = "cpu"
    if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        device = "mps"
    elif torch.cuda.is_available():
        device = "cuda"
    
    model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS", device=device)
    dimension = model.get_sentence_embedding_dimension()
    logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (device: {device}, dimension: {dimension})")
except Exception as e:
    logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    raise

# FAISS ì €ì¥ í´ë” ì´ˆê¸°í™”
if not os.path.exists(FAISS_STORE_DIR):
    os.makedirs(FAISS_STORE_DIR)

# ê¸°ì¡´ ì¸ë±ìŠ¤ í™•ì¸
if os.path.exists(INDEX_SAVE_PATH) and os.path.exists(META_SAVE_PATH):
    logger.warning("âš ï¸  ê¸°ì¡´ ì¸ë±ìŠ¤ íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤.")
    response = input("ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ë®ì–´ì“°ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
    if response.lower() != 'y':
        logger.info("ì‘ì—… ì·¨ì†Œë¨")
        exit(0)
    # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
    for path in [INDEX_SAVE_PATH, META_SAVE_PATH, LAST_PROCESSED_PATH]:
        if os.path.exists(path):
            os.remove(path)

try:
    # ë°ì´í„° ì—°ê²° ë° ë¡œë”©
    logger.info("ğŸ”Œ DB ì—°ê²° ì¤‘...")
    # ë¡œì»¬ ì‹¤í–‰ ì‹œì—ëŠ” ì§ì ‘ DB ì—°ê²° ìƒì„±
    from sqlalchemy import create_engine
    from sqlalchemy.orm import sessionmaker
    
    # ë¡œì»¬ ì‹¤í–‰ìš© DB URL (Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ê°€ ì•„ë‹Œ ê²½ìš°)
    import os
    DB_HOST = os.getenv("DB_HOST", "127.0.0.1")
    DB_PORT = os.getenv("DB_PORT", "3307")
    DB_USER = os.getenv("DB_USER", "root")
    DB_PASS = os.getenv("DB_PASS", "root")
    DB_NAME = os.getenv("DB_NAME", "recipe_db")
    
    DB_URL = f"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
    local_engine = create_engine(DB_URL)
    LocalSession = sessionmaker(bind=local_engine)
    session = LocalSession()
    
    # recipe_new í…Œì´ë¸”ì—ì„œ ë°ì´í„° ë¡œë“œ
    result = session.execute(text("""
        SELECT id, title, ingredients, tools, content, main_ingredients, sub_ingredients 
        FROM recipe_new
        ORDER BY id
    """))
    data = result.fetchall()
    logger.info(f"âœ… ì´ {len(data)}ê°œ ë ˆì‹œí”¼ ë¡œë”© ì™„ë£Œ")
    
    if len(data) == 0:
        logger.error("âŒ recipe_new í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        logger.info("ğŸ’¡ ë¨¼ì € load_csv_to_new_table.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì„¸ìš”.")
        session.close()
        exit(1)
    
    # í…ìŠ¤íŠ¸ ë³€í™˜ í•¨ìˆ˜ (ì£¼ì¬ë£Œ ê°•ì¡°)
    def recipe_to_text(row):
        """ë ˆì‹œí”¼ë¥¼ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì£¼ì¬ë£Œ ê°•ì¡°)"""
        title = str(row.title) if row.title else ""
        
        # ì£¼ì¬ë£Œê°€ ìˆìœ¼ë©´ ì£¼ì¬ë£Œë¥¼ ê°•ì¡°í•œ í…ìŠ¤íŠ¸ ìƒì„±
        if row.main_ingredients:
            main_list = str(row.main_ingredients).split(",")
            main_list = [m.strip() for m in main_list if m.strip()][:3]  # ìƒìœ„ 3ê°œ ì£¼ì¬ë£Œë§Œ
            if main_list:
                return f"{title} ë ˆì‹œí”¼ì˜ ì£¼ì¬ë£ŒëŠ” {', '.join(main_list)}ì…ë‹ˆë‹¤."
        
        # ì£¼ì¬ë£Œê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹
        if row.ingredients:
            ingredients = str(row.ingredients).split(",")[:5]  # ìƒìœ„ 5ê°œ ì¬ë£Œ
            ingredients = [i.strip() for i in ingredients if i.strip()]
            if ingredients:
                return f"{title} ë ˆì‹œí”¼ì˜ ì¬ë£ŒëŠ” {', '.join(ingredients)}ì…ë‹ˆë‹¤."
        
        # ì¬ë£Œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì œëª©ë§Œ
        return f"{title} ë ˆì‹œí”¼ì…ë‹ˆë‹¤."
    
    texts = [recipe_to_text(row) for row in data]
    
    # ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
    metadata = []
    
    # ì²˜ë¦¬ ì§€ì  ë¡œë“œ
    if os.path.exists(LAST_PROCESSED_PATH):
        with open(LAST_PROCESSED_PATH, "r") as f:
            last_processed = int(f.read().strip() or 0)
    else:
        last_processed = 0
    
    # ì¸ë±ìŠ¤ ì´ˆê¸°í™”
    logger.info("ğŸ“ ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„±")
    index = faiss.IndexFlatL2(dimension)
    
    # ë²¡í„°í™” ë° ì €ì¥ ë£¨í”„
    logger.info(f"ğŸ§  ì„ë² ë”© ì‹œì‘ (ì²˜ë¦¬ ì§€ì : {last_processed})...")
    
    for start in tqdm(range(last_processed, len(texts), CHUNK_SIZE), desc="ì„ë² ë”© ì§„í–‰"):
        end = min(start + CHUNK_SIZE, len(texts))
        text_chunk = texts[start:end]
        
        filtered_texts = []
        filtered_ids = []
        
        for i, text in enumerate(text_chunk):
            if isinstance(text, str) and len(text.strip()) > 0:
                filtered_texts.append(text)
                row = data[start + i]
                filtered_ids.append({
                    "id": row.id,
                    "title": row.title,
                    "ingredients": row.ingredients,
                    "main_ingredients": row.main_ingredients,
                    "sub_ingredients": row.sub_ingredients,
                    "content": row.content
                })
        
        if not filtered_texts:
            continue
        
        logger.info(f"ğŸ§  ì„ë² ë”© ì¤‘: {start} ~ {end} (ì´ {len(filtered_texts)}ê°œ)")
        
        try:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            
            emb_chunk = model.encode(filtered_texts, show_progress_bar=False)
            
            if emb_chunk.ndim != 2 or emb_chunk.shape[1] != dimension:
                logger.error(f"âŒ ì˜ëª»ëœ ë²¡í„° ì°¨ì›: {emb_chunk.shape}")
                continue
            
            index.add(np.array(emb_chunk).astype("float32"))
            metadata.extend(filtered_ids)
            
            # ì €ì¥
            os.makedirs(os.path.dirname(INDEX_SAVE_PATH), exist_ok=True)
            faiss.write_index(index, INDEX_SAVE_PATH)
            with open(META_SAVE_PATH, "wb") as f:
                pickle.dump(metadata, f)
            with open(LAST_PROCESSED_PATH, "w") as f:
                f.write(str(end))
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del emb_chunk
            gc.collect()
            
            logger.info(f"âœ… {end}/{len(texts)} ì™„ë£Œ (ì¸ë±ìŠ¤ í¬ê¸°: {index.ntotal})")
            
        except Exception as e:
            logger.exception(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {start}-{end} êµ¬ê°„ â†’ {str(e)}")
            break
    
    logger.info("=" * 60)
    logger.info("âœ… ì „ì²´ ì„ë² ë”© ë° ì €ì¥ ì™„ë£Œ!")
    logger.info(f"ğŸ“Š ì¸ë±ìŠ¤ í¬ê¸°: {index.ntotal}ê°œ")
    logger.info(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ:")
    logger.info(f"   - ì¸ë±ìŠ¤: {INDEX_SAVE_PATH}")
    logger.info(f"   - ë©”íƒ€ë°ì´í„°: {META_SAVE_PATH}")
    logger.info("=" * 60)

except Exception as e:
    logger.exception(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    raise

finally:
    # ì„¸ì…˜ ì¢…ë£Œ
    if 'session' in locals():
        session.close()
    logger.info("ğŸ”Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ")

